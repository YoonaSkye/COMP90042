{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic QA system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the files\n",
    "import json\n",
    "documents_set = json.load(open('documents.json'))\n",
    "training_set = json.load(open('training.json'))\n",
    "devel_set = json.load(open('devel.json'))\n",
    "testing_set = json.load(open('testing.json'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> 1. Information Retrieval to find the best matching sentence </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from math import log\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from collections import defaultdict, Counter\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"wordnet\") \n",
    "nltk.download('punkt')\n",
    "\n",
    "stopwords = set(nltk.corpus.stopwords.words('english')) \n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "lmtzr = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "\n",
    "\n",
    "# Preprocessing: lowercase, remove stopwords, stem the words and form words representation \n",
    "def preprocess(doc):\n",
    "    preprocessed_doc = []\n",
    "    doc = re.sub(r'[^\\w\\s]', '', doc) # remove punctuations\n",
    "    tokens = word_tokenize(doc)\n",
    "    for token in tokens:\n",
    "        if token.lower() not in stopwords: #remove stopwords\n",
    "            preprocessed_doc.append(stemmer.stem(token.lower())) # stem the word\n",
    "    return preprocessed_doc        \n",
    "\n",
    "# collect term frequencies for each sentence (a bag of words)\n",
    "def extract_term_freqs(sentence):\n",
    "    tfs = Counter()\n",
    "    preprocessed_sent = preprocess(sentence)\n",
    "    for token in preprocessed_sent:\n",
    "        tfs[token] += 1\n",
    "    return tfs\n",
    "\n",
    "# compute document frequencies(here refers to a term occurs in how many sentences within a document)\n",
    "def compute_doc_freqs(doc_term_freqs):\n",
    "    dfs = Counter()\n",
    "    for tfs in doc_term_freqs.values():\n",
    "        for term in tfs.keys():\n",
    "            dfs[term] += 1\n",
    "    return dfs\n",
    "\n",
    "# process documents_set into sentences\n",
    "def sent_tokenize(documents_set):\n",
    "    documents_set_sents = {}\n",
    "    for docid in range(len(documents_set)):\n",
    "        doc = documents_set[docid]['text']\n",
    "        sentences = []\n",
    "        for para in doc:\n",
    "            sentences += nltk.sent_tokenize(para)\n",
    "        documents_set_sents[docid] = sentences\n",
    "    return documents_set_sents\n",
    "documents_set_sents = sent_tokenize(documents_set)\n",
    "\n",
    "\n",
    "# process the document_set into term frequencies\n",
    "def get_term_frequencies(documents_set_sents):\n",
    "    doc_term_freqs = {}\n",
    "    for docid in range(len(documents_set_sents)):\n",
    "        doc = documents_set_sents[docid]\n",
    "        sent_term_freqs = {}\n",
    "        for sent_id in range(len(doc)):\n",
    "            term_freqs = extract_term_freqs(doc[sent_id])\n",
    "            sent_term_freqs[sent_id] = term_freqs\n",
    "        doc_term_freqs[docid] = sent_term_freqs\n",
    "    return doc_term_freqs\n",
    "doc_term_freqs = get_term_frequencies(documents_set_sents)\n",
    "\n",
    "\n",
    "# process the document_set into document frequencies\n",
    "def get_doc_freqs(doc_term_freqs):\n",
    "    doc_freqs = {}\n",
    "    for docid in doc_term_freqs.keys():\n",
    "        sent_freqs = compute_doc_freqs(doc_term_freqs[docid])\n",
    "        doc_freqs[docid] = sent_freqs\n",
    "    return doc_freqs\n",
    "doc_freqs = get_doc_freqs(doc_term_freqs)\n",
    "\n",
    "\n",
    "# build an inverted index to allow for efficient lookup by term\n",
    "def inverted_index(doc_term_freqs):\n",
    "    inverted_index_dict = {}\n",
    "    for docid in doc_term_freqs.keys():\n",
    "        inverted_index = defaultdict(list)\n",
    "        # note the inversion of the indexing, to be term -> (sent_id, tf)\n",
    "        for sent_id, term_freqs in doc_term_freqs[docid].items():\n",
    "            for term in term_freqs.keys():\n",
    "                inverted_index[term].append([sent_id, term_freqs[term]])\n",
    "        inverted_index_dict[docid] = inverted_index\n",
    "    return inverted_index_dict\n",
    "inverted_index_dict = inverted_index(doc_term_freqs)\n",
    "\n",
    "\n",
    "# Store the number of tokens in each sentence of a document\n",
    "def get_token_num(documents_set_sents):\n",
    "    token_num_dict = {}\n",
    "    for docid in range(len(documents_set_sents)):\n",
    "        sent_length_dict = {}\n",
    "        doc = documents_set_sents[docid]\n",
    "        for sent_id in range(len(doc)):\n",
    "            preprocessed_sent = preprocess(doc[sent_id])\n",
    "            sent_length_dict[sent_id] = len(preprocessed_sent)\n",
    "        token_num_dict[docid] = sent_length_dict\n",
    "    return token_num_dict\n",
    "token_num_dict = get_token_num(documents_set_sents)\n",
    "\n",
    "\n",
    "# store the number of sentences in a documents\n",
    "def get_sent_num(documents_set_sents):\n",
    "    sent_num = {}\n",
    "    for docid in range(len(documents_set_sents)):\n",
    "        sent_num[docid] = len(documents_set_sents[docid])\n",
    "    return sent_num\n",
    "sent_num = get_sent_num(documents_set_sents)\n",
    "\n",
    "\n",
    "# compute BM25\n",
    "def Okapi_BM25(query, docid, sent_num, inverted_index_dict, token_num_dict, doc_term_freqs, doc_freqs):\n",
    "    preprocessed_query = preprocess(query)\n",
    "    query_terms = set(preprocessed_query)\n",
    "    query_terms_freqs = extract_term_freqs(query)\n",
    "    k1 = 1.2\n",
    "    k2 = 1.5\n",
    "    b = 0.75\n",
    "    score = {}\n",
    "                \n",
    "    for sent_id in range(sent_num[docid]):\n",
    "        sent_score = 0\n",
    "        for term in query_terms:\n",
    "            N = sent_num[docid]                          # the number of sentences\n",
    "            f = len(inverted_index_dict[docid][term])    # number of sentences contain term\n",
    "            fdt =  doc_term_freqs[docid][sent_id][term]  # number of a term in a sentence\n",
    "            Ld = sum(doc_term_freqs[docid][sent_id].values())     # length of a sentence\n",
    "            Lavg =  sum(doc_freqs[docid].values()) / sent_num[docid]   # ave length of sentences\n",
    "            fqt = query_terms_freqs[term]                # number of term in a query\n",
    "\n",
    "            idf = log((N - f + 0.5)/(f + 0.5))\n",
    "            tf_doc = ((k1 + 1) * fdt)/(k1 * ((1-b) + b * Ld/Lavg) + fdt)\n",
    "            query_tf = (k2 + 1) * fqt / (k2 + fqt)\n",
    "            wt = idf * tf_doc * query_tf\n",
    "            sent_score += wt\n",
    "        score[sent_id] = sent_score\n",
    "    return score\n",
    "\n",
    "# return the best matching sentence\n",
    "def best_matching_sent_id(scores):\n",
    "    best_matching_sent_id = max(scores, key=lambda k: scores[k])\n",
    "    return best_matching_sent_id\n",
    "\n",
    "def best_match_sent(documents_set_sents, data_set):\n",
    "    best_match_sent = {}\n",
    "    for query_id in range(len(data_set)):\n",
    "        query = data_set[query_id]['question']\n",
    "        docid = data_set[query_id]['docid']\n",
    "        scores = Okapi_BM25(query, docid, sent_num, inverted_index_dict, token_num_dict, doc_term_freqs, doc_freqs)\n",
    "        best_matching_sent_id = max(scores, key=lambda k: scores[k])\n",
    "        best_match_sent[query_id] = documents_set_sents[docid][best_matching_sent_id]\n",
    "    return best_match_sent\n",
    "\n",
    "# generate best match sentence for questions in training set, development set and testing set.\n",
    "best_match_sent_train = best_match_sent(documents_set_sents, data_set = training_set)\n",
    "best_match_sent_devel = best_match_sent(documents_set_sents, data_set = devel_set)\n",
    "best_match_sent_test = best_match_sent(documents_set_sents, data_set = testing_set)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> 2. Named Entity Recognition</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Reference: Jenny Rose Finkel, Trond Grenager, and Christopher Manning. 2005. Incorporating Non-local Information into Information Extraction Systems \n",
    "by Gibbs Sampling. Proceedings of the 43nd Annual Meeting of the Association for Computational Linguistics (ACL 2005), \n",
    "pp. 363-370. http://nlp.stanford.edu/~manning/papers/gibbscrf3.pdf\n",
    "'''\n",
    "\n",
    "import nltk\n",
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk import pos_tag, sent_tokenize, word_tokenize\n",
    "from nltk.chunk import conlltags2tree\n",
    "from nltk.tree import Tree\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# StanfordNERTagger 7 class model for recognizing locations, persons, organizations, times, money, percents, and dates\n",
    "st = StanfordNERTagger('/Users/yue/stanford-ner/classifiers/english.muc.7class.distsim.crf.ser.gz',\n",
    "                       '/Users/yue/stanford-ner/stanford-ner.jar',\n",
    "                       encoding='utf-8')\n",
    "\n",
    "# obtain stanford entity information for best match sentences\n",
    "def get_tagged_sents(best_match_sents):\n",
    "    sents_list = []\n",
    "    for i in range(len(best_match_sents.keys())):\n",
    "        sents_list.append(word_tokenize(best_match_sents[i]))\n",
    "    tagged_sent = st.tag_sents(sents_list)\n",
    "    for st_tag in tagged_sent:\n",
    "        for token, tag in st_tag:\n",
    "            if token == '': # remove empty array\n",
    "                st_tag.remove((token, tag))\n",
    "    return tagged_sent\n",
    "        \n",
    "st_best_match_sents_devel = get_tagged_sents(best_match_sent_devel)\n",
    "st_best_match_sents_train = get_tagged_sents(best_match_sent_train)\n",
    "st_best_match_sents_test = get_tagged_sents(best_match_sent_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.chunk import conlltags2tree\n",
    "from nltk.tree import Tree\n",
    "\n",
    "# convert Stanford NER Tagger result into NLTK tress\n",
    "## adapted from https://stackoverflow.com/questions/30664677/extract-list-of-persons-and-organizations-using-stanford-ner-tagger-in-nltk\n",
    "# word boundaries\n",
    "def stanfordNE2bio(st_tag):\n",
    "    bio_tagged_sent = []\n",
    "    prev_tag = 'O'\n",
    "    for token, tag in st_tag:\n",
    "        if tag == \"O\": #O\n",
    "            bio_tagged_sent.append((token, tag))\n",
    "            prev_tag = tag\n",
    "            continue\n",
    "        if tag != \"O\" and prev_tag == \"O\": # Begin NE\n",
    "            bio_tagged_sent.append((token, \"B-\"+tag))\n",
    "            prev_tag = tag\n",
    "        elif prev_tag != \"O\" and prev_tag == tag: # Inside NE\n",
    "            bio_tagged_sent.append((token, \"I-\"+tag))\n",
    "            prev_tag = tag\n",
    "        elif prev_tag != \"O\" and prev_tag != tag: # Adjacent NE\n",
    "            bio_tagged_sent.append((token, \"B-\"+tag))\n",
    "            prev_tag = tag\n",
    "    return bio_tagged_sent\n",
    "\n",
    "# convert Stanford NER Tagger result into NLTK tress\n",
    "def stanfordNE2tree(st_tag):\n",
    "    bio_tagged_sent = stanfordNE2bio(st_tag)\n",
    "    sent_tokens, sent_ne_tags = zip(*bio_tagged_sent)\n",
    "    sent_pos_tags = [pos for token, pos in pos_tag(sent_tokens)]\n",
    "    sent_conlltags = [(token, pos, ne) for token, pos, ne in zip(sent_tokens, sent_pos_tags, sent_ne_tags)]\n",
    "    ne_tree = conlltags2tree(sent_conlltags)\n",
    "    return ne_tree\n",
    "\n",
    "# get continuous named entity words\n",
    "def get_continuous_NE(st_tag):\n",
    "    ne_tree = stanfordNE2tree(st_tag)\n",
    "    ne_in_sent = []\n",
    "    ne_token = []\n",
    "    for subtree in ne_tree:\n",
    "        if type(subtree) == Tree: \n",
    "            ne_label = subtree.label()\n",
    "            ne_string = \" \".join([token for token, pos in subtree.leaves()])\n",
    "            ne_in_sent.append((ne_string.lower(), ne_label))\n",
    "            ne_token.append(ne_string.lower())\n",
    "    return ne_in_sent\n",
    "\n",
    "## End of adaptation\n",
    "\n",
    "# obtain named entity for best matching sent by joining contiguous words with the same type for each data set\n",
    "def get_final_en(st_best_match_sents):\n",
    "    final_en = []\n",
    "    for st_tag in st_best_match_sents:\n",
    "        final_en.append(get_continuous_NE(st_tag))\n",
    "    return final_en\n",
    "\n",
    "devel_set_ne = get_final_en(st_best_match_sents_devel)\n",
    "train_set_ne = get_final_en(st_best_match_sents_train)\n",
    "test_set_ne = get_final_en(st_best_match_sents_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> 3. Question type classification and answer extraction </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer type detection\n",
    "'''\n",
    "question wh-word: what, which, when, where, who, why, how(how far, how many, how much, how long).\n",
    "\n",
    "Qestion Class  Answer type\n",
    "Who/Whom       (Person/Organisation)\n",
    "Where          (Location)\n",
    "When           (Date/Time)\n",
    "Which          (Person/Location/Date/Time)\n",
    "Why            (Unkown)\n",
    "What           (Locations, Persons, Organizations, Times, Money, Percents, Dates)\n",
    "How            (Money, Percents, etc)\n",
    "'''\n",
    "answers = []\n",
    "same_count = 0\n",
    "for question_id in range(len(testing_set)):\n",
    "    ans = 'unk'\n",
    "    candidate_ans = []\n",
    "    lowercased_query = testing_set[question_id]['question'].lower()\n",
    "    # best match sentence\n",
    "    best_match_sentence = best_match_sent_test[question_id]\n",
    "    # named entity for best match sentence\n",
    "    final_ne_sent = test_set_ne[question_id]\n",
    "    sent_pos = [pos for token,pos in final_ne_sent]\n",
    "    # question answering\n",
    "    if final_ne_sent == []:\n",
    "        ans = 'unk'\n",
    "    else:\n",
    "        # when\n",
    "        if 'when' in lowercased_query or 'date' in lowercased_query or 'year' in lowercased_query:\n",
    "            for token, pos in final_ne_sent:\n",
    "                if pos == 'DATE' or pos == 'TIME':\n",
    "                    ans = token\n",
    "\n",
    "        # who\n",
    "        elif 'who' in lowercased_query or 'whom' in lowercased_query:\n",
    "            for token, pos in final_ne_sent: \n",
    "                if pos == 'ORGANISATION':\n",
    "                    ans = token\n",
    "                elif pos == 'PERSON':\n",
    "                    ans = token\n",
    "\n",
    "        # where\n",
    "        elif 'where' in lowercased_query or 'place' in lowercased_query:\n",
    "            for token, pos in final_ne_sent:\n",
    "                if pos == 'LOCATION':\n",
    "                    ans = token\n",
    "\n",
    "        # num: how far, how many, how much, how long\n",
    "        elif 'how' in lowercased_query or 'far' in lowercased_query or 'many' in lowercased_query or 'much' in lowercased_query or 'long' in lowercased_query:\n",
    "            for token, pos in final_ne_sent:\n",
    "                if pos == 'MONEY':\n",
    "                    ans = token\n",
    "                elif pos == 'PERCENT':\n",
    "                    ans = token\n",
    "                elif pos == 'TIME':\n",
    "                    ans = token\n",
    "                elif pos == 'DATE':\n",
    "                    ans = token\n",
    "                    \n",
    "        elif 'percent' in lowercased_query:\n",
    "            for token, pos in final_ne_sent:\n",
    "                if pos == 'PERCENT':\n",
    "                    ans = token\n",
    "\n",
    "        if ans == 'unk':\n",
    "            ans = final_ne_sent[0][0]\n",
    "    answers.append([question_id, ans])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('test_ans1.csv', 'w', newline='', encoding='UTF-8') as csvfile:\n",
    "    writer = csv.writer(csvfile, delimiter=',')\n",
    "    header = ['id', 'answer']\n",
    "    writer.writerow(header)\n",
    "    writer.writerows(answers)\n",
    "csvfile.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
